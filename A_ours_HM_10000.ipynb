{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_AUGMENTATION=True\n",
    "AUGMENTATION_RATIO=1\n",
    "batch_size=32\n",
    "E=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 17:01:06.115987: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-21 17:01:06.137973: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-21 17:01:06.137991: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-21 17:01:06.138004: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-21 17:01:06.142240: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-21 17:01:06.142642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-21 17:01:06.872041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow.keras.applications import DenseNet121,VGG16\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# from data_processing import data_processing\n",
    "from data_2 import data_simulate_noniid\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x,train_y,train_s, vali_x, vali_y, vali_s, train_x_1, vali_x_1, train_y_1, vali_y_1, train_s_1, vali_s_1, train_x_2, vali_x_2, train_y_2, vali_y_2, train_s_2, vali_s_2, train_x_3, vali_x_3, train_y_3, vali_y_3, train_s_3, vali_s_3, train_x_4, vali_x_4, train_y_4, vali_y_4, train_s_4, vali_s_4, train_x_5, vali_x_5, train_y_5, vali_y_5, train_s_5, vali_s_5=data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.3, 0.5, 0.7, 0.9]\n",
      "(750, 2352) (750, 2352) (750, 2352) (750, 2352) (749, 2352)\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,train_s, vali_x, vali_y, vali_s, train_x_1, vali_x_1, train_y_1, vali_y_1, train_s_1, vali_s_1, train_x_2, vali_x_2, train_y_2, vali_y_2, train_s_2, vali_s_2, train_x_3, vali_x_3, train_y_3, vali_y_3, train_s_3, vali_s_3, train_x_4, vali_x_4, train_y_4, vali_y_4, train_s_4, vali_s_4, train_x_5, vali_x_5, train_y_5, vali_y_5, train_s_5, vali_s_5=data_simulate_noniid(settings=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "def create_CNN_model(): \n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare  data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 749\n",
      "Images in training dataset after augmentation: 1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 17:01:09.252846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-21 17:01:09.269125: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset, is_training):\n",
    "    dataset = dataset.cache().shuffle(10, reshuffle_each_iteration = False)\n",
    "    \n",
    "    if is_training == True and IMAGE_AUGMENTATION == True:\n",
    "        print(\"Images in training dataset before augmentation: \" + str(len(dataset)))\n",
    "        dataset_augmented = dataset.take(int(AUGMENTATION_RATIO*len(dataset))).map(augment, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.concatenate(dataset_augmented)\n",
    "        print(\"Images in training dataset after augmentation: \" + str(len(dataset)))\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def augment(image, label_y):\n",
    "    # image = tf.image.rotate(image, random.uniform(-10, 10)*math.pi/180)\n",
    "    image = tf.image.central_crop(image, random.uniform(0.9, 1.0))\n",
    "    image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n",
    "    image = tf.image.resize(image, [28, 28])\n",
    "    \n",
    "    return image, label_y\n",
    "def data_augmentation(train_x,train_y):\n",
    "    training_set = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    training_set = preprocess_dataset(training_set, is_training = True)\n",
    "    return training_set\n",
    "# def data_augmentation(train_x, train_y):\n",
    "#     dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(batch_size=batch_size)\n",
    "#     return dataset\n",
    "training_set_1=data_augmentation(train_x_1, train_y_1)\n",
    "training_set_2=data_augmentation(train_x_2, train_y_2)\n",
    "training_set_3=data_augmentation(train_x_3, train_y_3)\n",
    "training_set_4=data_augmentation(train_x_4, train_y_4)\n",
    "training_set_5=data_augmentation(train_x_5, train_y_5)\n",
    "p_1=len(training_set_1)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_2=len(training_set_2)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_3=len(training_set_3)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_4=len(training_set_4)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_5=len(training_set_5)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "def create_CNN_model(): \n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def multiply(weights,c):\n",
    "\n",
    "    new_weights = []\n",
    "    for i in range(len(weights)):\n",
    "        new_weights.append(weights[i]*c)\n",
    "    return new_weights\n",
    "\n",
    "# def norm_square(weights_n):\n",
    "#     sum = 0\n",
    "#     for i in range(len(weights_n)):\n",
    "#         sum += np.linalg.norm(weights_n[i])**2\n",
    "#     return sum\n",
    "def model_weight_aggregation(weight_1, weight_2, weight_3, weight_4, weight_5):\n",
    "    added_weight = []\n",
    "    weight_1=multiply(weight_1,p_1)\n",
    "    weight_2=multiply(weight_2,p_2)\n",
    "    weight_3=multiply(weight_3,p_3)\n",
    "    weight_4=multiply(weight_4,p_4)\n",
    "    weight_5=multiply(weight_5,p_5)\n",
    "    for i in range(len(weight_1)):\n",
    "        added_weight.append(weight_1[i]+weight_2[i]+weight_3[i]+weight_4[i]+weight_5[i])\n",
    "    return added_weight\n",
    "def model_weight_minus(weight_1, weight_2):\n",
    "\n",
    "    minus_weights = []\n",
    "    for i in range(len(weight_1)):\n",
    "        minus_weights.append(weight_1[i] - weight_2[i])\n",
    "    return minus_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a FedAvg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_model=create_CNN_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model=create_CNN_model()\n",
    "w_0=global_model.get_weights()\n",
    "global_weight=[]\n",
    "global_weight.append(w_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_set,loacl_epoch):\n",
    "    w_t=global_weight[0]\n",
    "    clients_model.set_weights(w_t)\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    clients_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer =optimizer,\n",
    "                  metrics = ['accuracy'])\n",
    "    history =clients_model.fit(training_set,\n",
    "                    epochs = 5)\n",
    "    return clients_model.get_weights()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fedavg():\n",
    "    w_t_1=train_model(training_set_1,E)\n",
    "    w_t_2=train_model(training_set_2,E)\n",
    "    w_t_3=train_model(training_set_3,E)\n",
    "    w_t_4=train_model(training_set_4,E)\n",
    "    w_t_5=train_model(training_set_5,E)\n",
    "    new_w=model_weight_aggregation(w_t_1,w_t_2,w_t_3,w_t_4,w_t_5)\n",
    "    global_weight[0]=new_w\n",
    "    global_model.set_weights(new_w)\n",
    "    return new_w\n",
    "def acc_calculator(y_true, y_pred):\n",
    "    accuracy_count=np.where(y_true==y_pred, 1.0, 0.0)\n",
    "    accuracy=sum(accuracy_count)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8708 - accuracy: 0.7220\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7485 - accuracy: 0.7227\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7287 - accuracy: 0.7227\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7196 - accuracy: 0.7227\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7164 - accuracy: 0.7227\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.9459 - accuracy: 0.6627\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8456 - accuracy: 0.6640\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8250 - accuracy: 0.6640\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8155 - accuracy: 0.6640\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8026 - accuracy: 0.6640\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.9256 - accuracy: 0.6760\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8189 - accuracy: 0.6773\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7918 - accuracy: 0.6773\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7809 - accuracy: 0.6773\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7847 - accuracy: 0.6773\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.9777 - accuracy: 0.6373\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8598 - accuracy: 0.6373\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8250 - accuracy: 0.6360\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8128 - accuracy: 0.6380\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8053 - accuracy: 0.6453\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.9450 - accuracy: 0.6442\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8305 - accuracy: 0.6435\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7901 - accuracy: 0.6402\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7867 - accuracy: 0.6409\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7717 - accuracy: 0.6429\n",
      "9.45316481590271\n",
      "vali_acc= 0.6626666666666666\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7343 - accuracy: 0.7227\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7099 - accuracy: 0.7227\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7057 - accuracy: 0.7227\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6955 - accuracy: 0.7233\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6867 - accuracy: 0.7253\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8205 - accuracy: 0.6640\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8029 - accuracy: 0.6640\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7911 - accuracy: 0.6647\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7768 - accuracy: 0.6700\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7568 - accuracy: 0.6847\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8037 - accuracy: 0.6773\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7787 - accuracy: 0.6773\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7718 - accuracy: 0.6773\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7686 - accuracy: 0.6773\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7561 - accuracy: 0.6773\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8183 - accuracy: 0.6453\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7929 - accuracy: 0.6520\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7785 - accuracy: 0.6587\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7605 - accuracy: 0.6753\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7431 - accuracy: 0.6867\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8032 - accuracy: 0.6469\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7736 - accuracy: 0.6489\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7633 - accuracy: 0.6489\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7512 - accuracy: 0.6542\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7370 - accuracy: 0.6569\n",
      "9.337708234786987\n",
      "vali_acc= 0.6738666666666666\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6988 - accuracy: 0.7233\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.7240\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6632 - accuracy: 0.7380\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6555 - accuracy: 0.7440\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6341 - accuracy: 0.7587\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7811 - accuracy: 0.6687\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7485 - accuracy: 0.6747\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7308 - accuracy: 0.6907\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7140 - accuracy: 0.6920\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6956 - accuracy: 0.7060\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.7659 - accuracy: 0.6773\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7407 - accuracy: 0.6740\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7205 - accuracy: 0.6807\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.6880\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7050 - accuracy: 0.6887\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7696 - accuracy: 0.6707\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7311 - accuracy: 0.6967\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7057 - accuracy: 0.7013\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6851 - accuracy: 0.7073\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6706 - accuracy: 0.7140\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7534 - accuracy: 0.6522\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7294 - accuracy: 0.6569\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.7182 - accuracy: 0.6729\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7066 - accuracy: 0.6802\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.6996\n",
      "9.391966104507446\n",
      "vali_acc= 0.6738666666666666\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.7447\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6387 - accuracy: 0.7480\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6231 - accuracy: 0.7553\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6111 - accuracy: 0.7613\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5970 - accuracy: 0.7707\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7195 - accuracy: 0.7000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.7140\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6745 - accuracy: 0.7180\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6751 - accuracy: 0.7167\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6504 - accuracy: 0.7200\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7240 - accuracy: 0.6927\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.7067\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.6993\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6796 - accuracy: 0.7053\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6543 - accuracy: 0.7187\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.7017 - accuracy: 0.6980\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6655 - accuracy: 0.7180\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6527 - accuracy: 0.7227\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6422 - accuracy: 0.7240\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6411 - accuracy: 0.7273\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7037 - accuracy: 0.6802\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.6943\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6694 - accuracy: 0.7009\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6578 - accuracy: 0.7163\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6347 - accuracy: 0.7150\n",
      "9.510842323303223\n",
      "vali_acc= 0.708\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6269 - accuracy: 0.7427\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 0.7667\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5790 - accuracy: 0.7767\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5763 - accuracy: 0.7720\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5545 - accuracy: 0.7800\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.7173\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6641 - accuracy: 0.7207\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6416 - accuracy: 0.7227\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6300 - accuracy: 0.7373\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6251 - accuracy: 0.7320\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7060 - accuracy: 0.7020\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6520 - accuracy: 0.7173\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6506 - accuracy: 0.7273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6497 - accuracy: 0.7153\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6240 - accuracy: 0.7307\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6796 - accuracy: 0.7113\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6283 - accuracy: 0.7327\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6165 - accuracy: 0.7460\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6136 - accuracy: 0.7507\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5965 - accuracy: 0.7533\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6706 - accuracy: 0.7063\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6625 - accuracy: 0.7063\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6464 - accuracy: 0.7223\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6226 - accuracy: 0.7276\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6068 - accuracy: 0.7290\n",
      "10.0042724609375\n",
      "vali_acc= 0.7338666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6023 - accuracy: 0.7567\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5708 - accuracy: 0.7767\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.5461 - accuracy: 0.7807\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.7920\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5212 - accuracy: 0.7967\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6590 - accuracy: 0.7313\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.6315 - accuracy: 0.7320\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.6220 - accuracy: 0.7407\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6113 - accuracy: 0.7427\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.6002 - accuracy: 0.7447\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 8ms/step - loss: 0.6802 - accuracy: 0.7160\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6322 - accuracy: 0.7307\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6228 - accuracy: 0.7307\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6201 - accuracy: 0.7420\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5967 - accuracy: 0.7553\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6535 - accuracy: 0.7187\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6024 - accuracy: 0.7540\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5927 - accuracy: 0.7613\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.7607\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5720 - accuracy: 0.7647\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6532 - accuracy: 0.7143\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.6245 - accuracy: 0.7176\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6235 - accuracy: 0.7256\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6063 - accuracy: 0.7377\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5859 - accuracy: 0.7363\n",
      "10.438963651657104\n",
      "vali_acc= 0.7472\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5833 - accuracy: 0.7700\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5310 - accuracy: 0.7987\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5219 - accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.4905 - accuracy: 0.8107\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4762 - accuracy: 0.8140\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6381 - accuracy: 0.7360\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6036 - accuracy: 0.7467\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.7473\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5774 - accuracy: 0.7520\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5766 - accuracy: 0.7600\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6600 - accuracy: 0.7213\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6180 - accuracy: 0.7407\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5958 - accuracy: 0.7560\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5859 - accuracy: 0.7580\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5745 - accuracy: 0.7600\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6374 - accuracy: 0.7293\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5863 - accuracy: 0.7473\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5715 - accuracy: 0.7687\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5592 - accuracy: 0.7613\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5587 - accuracy: 0.7620\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6440 - accuracy: 0.7190\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6063 - accuracy: 0.7390\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5978 - accuracy: 0.7290\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5861 - accuracy: 0.7356\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5553 - accuracy: 0.7637\n",
      "9.467905521392822\n",
      "vali_acc= 0.7576\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5513 - accuracy: 0.7873\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5146 - accuracy: 0.7947\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4937 - accuracy: 0.8093\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4814 - accuracy: 0.8093\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4693 - accuracy: 0.8207\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 8ms/step - loss: 0.6214 - accuracy: 0.7400\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5861 - accuracy: 0.7453\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5726 - accuracy: 0.7600\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.5614 - accuracy: 0.7633\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5454 - accuracy: 0.7693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6426 - accuracy: 0.7193\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5938 - accuracy: 0.7547\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5812 - accuracy: 0.7587\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5548 - accuracy: 0.7740\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5575 - accuracy: 0.7787\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6220 - accuracy: 0.7353\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5689 - accuracy: 0.7600\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5539 - accuracy: 0.7667\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5549 - accuracy: 0.7687\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5338 - accuracy: 0.7840\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6254 - accuracy: 0.7316\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5918 - accuracy: 0.7530\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.7577\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5397 - accuracy: 0.7710\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5389 - accuracy: 0.7543\n",
      "9.79189157485962\n",
      "vali_acc= 0.7613333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5365 - accuracy: 0.7880\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4828 - accuracy: 0.8113\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4626 - accuracy: 0.8180\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4568 - accuracy: 0.8207\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4439 - accuracy: 0.8327\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6097 - accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5599 - accuracy: 0.7727\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5557 - accuracy: 0.7613\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5326 - accuracy: 0.7800\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5365 - accuracy: 0.7707\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6280 - accuracy: 0.7347\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5701 - accuracy: 0.7587\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5581 - accuracy: 0.7660\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5424 - accuracy: 0.7833\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5281 - accuracy: 0.7840\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6089 - accuracy: 0.7520\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5486 - accuracy: 0.7793\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5323 - accuracy: 0.7753\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5173 - accuracy: 0.7840\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5052 - accuracy: 0.7933\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5850 - accuracy: 0.7530\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5417 - accuracy: 0.7677\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5230 - accuracy: 0.7737\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5044 - accuracy: 0.7917\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5037 - accuracy: 0.7810\n",
      "9.608633041381836\n",
      "vali_acc= 0.7616\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5164 - accuracy: 0.7993\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4631 - accuracy: 0.8140\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4547 - accuracy: 0.8213\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4299 - accuracy: 0.8380\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4278 - accuracy: 0.8280\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5931 - accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5394 - accuracy: 0.7747\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5410 - accuracy: 0.7647\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5285 - accuracy: 0.7793\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5052 - accuracy: 0.7887\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6176 - accuracy: 0.7313\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5500 - accuracy: 0.7727\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5411 - accuracy: 0.7753\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5308 - accuracy: 0.7873\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5186 - accuracy: 0.7967\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5943 - accuracy: 0.7567\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5382 - accuracy: 0.7713\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5244 - accuracy: 0.7807\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5003 - accuracy: 0.7947\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4881 - accuracy: 0.8033\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5716 - accuracy: 0.7637\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5438 - accuracy: 0.7603\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5314 - accuracy: 0.7670\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4951 - accuracy: 0.7850\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4856 - accuracy: 0.7897\n",
      "9.308915853500366\n",
      "vali_acc= 0.7626666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5013 - accuracy: 0.8053\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4538 - accuracy: 0.8200\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4254 - accuracy: 0.8413\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4196 - accuracy: 0.8373\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4018 - accuracy: 0.8480\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5832 - accuracy: 0.7640\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5287 - accuracy: 0.7847\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5225 - accuracy: 0.7813\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5193 - accuracy: 0.7833\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4922 - accuracy: 0.8113\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6061 - accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5297 - accuracy: 0.7947\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5303 - accuracy: 0.7827\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5107 - accuracy: 0.7960\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4818 - accuracy: 0.8113\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5680 - accuracy: 0.7613\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7960\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4775 - accuracy: 0.8053\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4631 - accuracy: 0.8100\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5532 - accuracy: 0.7644\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5596 - accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5492 - accuracy: 0.7590\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4922 - accuracy: 0.7971\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4746 - accuracy: 0.8064\n",
      "9.540876150131226\n",
      "vali_acc= 0.7696\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4854 - accuracy: 0.8100\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4316 - accuracy: 0.8353\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4304 - accuracy: 0.8273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3912 - accuracy: 0.8487\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4004 - accuracy: 0.8500\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5765 - accuracy: 0.7593\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5048 - accuracy: 0.7880\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4937 - accuracy: 0.7887\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4898 - accuracy: 0.7980\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4870 - accuracy: 0.8020\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5880 - accuracy: 0.7507\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5169 - accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5130 - accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4706 - accuracy: 0.8140\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4636 - accuracy: 0.8173\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5483 - accuracy: 0.7653\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4966 - accuracy: 0.7967\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4731 - accuracy: 0.7993\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4644 - accuracy: 0.8140\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4479 - accuracy: 0.8173\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5482 - accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5320 - accuracy: 0.7764\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5004 - accuracy: 0.7891\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4898 - accuracy: 0.7917\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4747 - accuracy: 0.7891\n",
      "9.459570169448853\n",
      "vali_acc= 0.7701333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4634 - accuracy: 0.8073\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4160 - accuracy: 0.8387\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3929 - accuracy: 0.8520\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3846 - accuracy: 0.8533\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3527 - accuracy: 0.8740\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5497 - accuracy: 0.7667\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5041 - accuracy: 0.7973\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4934 - accuracy: 0.8053\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4729 - accuracy: 0.8053\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4388 - accuracy: 0.8260\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5756 - accuracy: 0.7673\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5053 - accuracy: 0.7967\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4788 - accuracy: 0.8133\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4692 - accuracy: 0.8093\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4800 - accuracy: 0.7953\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5339 - accuracy: 0.7653\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4793 - accuracy: 0.8040\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4575 - accuracy: 0.8160\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4428 - accuracy: 0.8193\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4167 - accuracy: 0.8333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5278 - accuracy: 0.7850\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4985 - accuracy: 0.7884\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4564 - accuracy: 0.7957\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4626 - accuracy: 0.8011\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4330 - accuracy: 0.8017\n",
      "9.704301357269287\n",
      "vali_acc= 0.7570666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4375 - accuracy: 0.8307\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3887 - accuracy: 0.8527\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3702 - accuracy: 0.8573\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3520 - accuracy: 0.8647\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3386 - accuracy: 0.8673\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5400 - accuracy: 0.7760\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4711 - accuracy: 0.8040\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4476 - accuracy: 0.8220\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4315 - accuracy: 0.8300\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4139 - accuracy: 0.8373\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5544 - accuracy: 0.7753\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5155 - accuracy: 0.7893\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4660 - accuracy: 0.8127\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4606 - accuracy: 0.8160\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4215 - accuracy: 0.8353\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5012 - accuracy: 0.7907\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4578 - accuracy: 0.8020\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4371 - accuracy: 0.8153\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4156 - accuracy: 0.8247\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3817 - accuracy: 0.8393\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5203 - accuracy: 0.7804\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4724 - accuracy: 0.7957\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4612 - accuracy: 0.8004\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4331 - accuracy: 0.8057\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4383 - accuracy: 0.8037\n",
      "9.354862689971924\n",
      "vali_acc= 0.7626666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4365 - accuracy: 0.8280\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3726 - accuracy: 0.8527\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3507 - accuracy: 0.8700\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3297 - accuracy: 0.8813\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3213 - accuracy: 0.8813\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5311 - accuracy: 0.7867\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4684 - accuracy: 0.8093\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4213 - accuracy: 0.8320\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4006 - accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3923 - accuracy: 0.8460\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5418 - accuracy: 0.7787\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4470 - accuracy: 0.8233\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4411 - accuracy: 0.8253\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4159 - accuracy: 0.8300\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4037 - accuracy: 0.8400\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7847\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4553 - accuracy: 0.8180\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4089 - accuracy: 0.8287\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3869 - accuracy: 0.8447\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3863 - accuracy: 0.8400\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5014 - accuracy: 0.7904\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4641 - accuracy: 0.7891\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.8111\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4392 - accuracy: 0.7991\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4147 - accuracy: 0.8104\n",
      "9.150716304779053\n",
      "vali_acc= 0.7744\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4051 - accuracy: 0.8427\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3562 - accuracy: 0.8587\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3482 - accuracy: 0.8660\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3209 - accuracy: 0.8740\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3025 - accuracy: 0.8833\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5166 - accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4340 - accuracy: 0.8293\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4078 - accuracy: 0.8347\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3780 - accuracy: 0.8460\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3705 - accuracy: 0.8427\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5355 - accuracy: 0.7747\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4355 - accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4248 - accuracy: 0.8367\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4081 - accuracy: 0.8340\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3745 - accuracy: 0.8567\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4858 - accuracy: 0.7947\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4174 - accuracy: 0.8320\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3958 - accuracy: 0.8347\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3605 - accuracy: 0.8567\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3410 - accuracy: 0.8633\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4993 - accuracy: 0.7891\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4415 - accuracy: 0.8077\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4067 - accuracy: 0.8284\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3937 - accuracy: 0.8338\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3957 - accuracy: 0.8264\n",
      "9.139231204986572\n",
      "vali_acc= 0.7656\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4137 - accuracy: 0.8320\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3529 - accuracy: 0.8647\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3317 - accuracy: 0.8707\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2950 - accuracy: 0.8887\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2660 - accuracy: 0.8940\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5151 - accuracy: 0.7873\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4159 - accuracy: 0.8420\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3881 - accuracy: 0.8400\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3649 - accuracy: 0.8600\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3322 - accuracy: 0.8680\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5187 - accuracy: 0.7947\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4223 - accuracy: 0.8300\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3910 - accuracy: 0.8413\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3795 - accuracy: 0.8580\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3451 - accuracy: 0.8693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4677 - accuracy: 0.8007\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3853 - accuracy: 0.8427\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3622 - accuracy: 0.8553\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3475 - accuracy: 0.8607\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3385 - accuracy: 0.8620\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4830 - accuracy: 0.8017\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4271 - accuracy: 0.8124\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4059 - accuracy: 0.8124\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3756 - accuracy: 0.8425\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3482 - accuracy: 0.8565\n",
      "9.140885353088379\n",
      "vali_acc= 0.7669333333333334\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.3985 - accuracy: 0.8420\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8780\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2846 - accuracy: 0.8953\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2696 - accuracy: 0.8940\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2468 - accuracy: 0.9113\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4886 - accuracy: 0.8013\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3956 - accuracy: 0.8433\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3596 - accuracy: 0.8527\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8760\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3136 - accuracy: 0.8773\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5021 - accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4012 - accuracy: 0.8420\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3758 - accuracy: 0.8507\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3340 - accuracy: 0.8680\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3310 - accuracy: 0.8700\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4448 - accuracy: 0.8120\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3930 - accuracy: 0.8367\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3780 - accuracy: 0.8420\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3283 - accuracy: 0.8693\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3207 - accuracy: 0.8747\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4808 - accuracy: 0.7937\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4319 - accuracy: 0.8131\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3986 - accuracy: 0.8351\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3541 - accuracy: 0.8445\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3467 - accuracy: 0.8505\n",
      "9.370593309402466\n",
      "vali_acc= 0.7709333333333334\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.3807 - accuracy: 0.8473\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2972 - accuracy: 0.8860\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2743 - accuracy: 0.8940\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2571 - accuracy: 0.9033\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2664 - accuracy: 0.9000\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4757 - accuracy: 0.8073\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3833 - accuracy: 0.8433\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3254 - accuracy: 0.8733\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3072 - accuracy: 0.8833\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2988 - accuracy: 0.8807\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4905 - accuracy: 0.8033\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3783 - accuracy: 0.8533\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3579 - accuracy: 0.8587\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3279 - accuracy: 0.8740\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3396 - accuracy: 0.8647\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4232 - accuracy: 0.8173\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3484 - accuracy: 0.8553\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3168 - accuracy: 0.8660\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3044 - accuracy: 0.8847\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2995 - accuracy: 0.8860\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4613 - accuracy: 0.8077\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3934 - accuracy: 0.8224\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3771 - accuracy: 0.8364\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3534 - accuracy: 0.8551\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3359 - accuracy: 0.8585\n",
      "10.187140941619873\n",
      "vali_acc= 0.7704\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.3542 - accuracy: 0.8587\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2937 - accuracy: 0.8827\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2592 - accuracy: 0.8980\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9060\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2158 - accuracy: 0.9193\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 8ms/step - loss: 0.4587 - accuracy: 0.8113\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3644 - accuracy: 0.8473\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.3009 - accuracy: 0.8820\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2918 - accuracy: 0.8827\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.2559 - accuracy: 0.9020\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5014 - accuracy: 0.7873\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3736 - accuracy: 0.8480\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3298 - accuracy: 0.8793\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3017 - accuracy: 0.8793\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3044 - accuracy: 0.8847\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4061 - accuracy: 0.8233\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3380 - accuracy: 0.8533\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3266 - accuracy: 0.8687\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3466 - accuracy: 0.8580\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3319 - accuracy: 0.8747\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4742 - accuracy: 0.8031\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3648 - accuracy: 0.8418\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3529 - accuracy: 0.8511\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3476 - accuracy: 0.8418\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3250 - accuracy: 0.8598\n",
      "9.785426139831543\n",
      "vali_acc= 0.7608\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "vali_accuracy=[]\n",
    "time_arr=[]\n",
    "for m in range (20):\n",
    "    y_pred=global_model(vali_x)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    acc=acc_calculator(vali_y,y_pred)\n",
    "    vali_accuracy.append(acc)\n",
    "    start=time.time()\n",
    "    w=Fedavg()\n",
    "    end=time.time()\n",
    "    global_model.set_weights(w)\n",
    "    time_count=end-start\n",
    "    time_arr.append(time_count)\n",
    "    print(end-start)\n",
    "    print('vali_acc=',acc)\n",
    "plt.plot(vali_accuracy,'b--',label='FedAvg')\n",
    "plt.xlabel('# of rounds')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "# print('The avg time for per communication'+str(np.mean(time_arr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and fairness of FedAvg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EO_compuatation(y_pred, y_ture,s_sensitive):\n",
    "    EO_list=[]\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    y_ture=np.reshape(y_ture,(-1,))\n",
    "    s_sensitive=np.reshape(s_sensitive,(-1,))\n",
    "    for i in range (4):\n",
    "        tpr_sensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==1),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==1),1,0))\n",
    "        tpr_nonsensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==0),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==0),1,0))\n",
    "        EO=abs(tpr_sensitive-tpr_nonsensitive)\n",
    "        if EO<1 or EO==1:\n",
    "            EO_list.append(EO)\n",
    "        else:\n",
    "            EO_list.append(0.01)\n",
    "    print(EO_list)\n",
    "    EO=np.max(EO_list)\n",
    "    return EO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.043523901808785626, 0.04960709170440908, 0.04934021801491684, 0.040000000000000036]\n",
      "global_EO= 0.04960709170440908\n",
      "global_acc= 0.7704\n"
     ]
    }
   ],
   "source": [
    "y_pred=global_model(vali_x)\n",
    "y_pred=tf.argmax(y_pred, axis=1)\n",
    "vali_EO=EO_compuatation(y_pred, vali_y,vali_s)\n",
    "print('global_EO=',vali_EO)\n",
    "y_pred=global_model(vali_x)\n",
    "y_pred=tf.argmax(y_pred, axis=1)\n",
    "acc_count=np.where(y_pred==vali_y,1,0)\n",
    "acc=sum(acc_count)/len(vali_y)\n",
    "print('global_acc=',acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10520361990950222, 0.19069069069069072, 0.015232484746475983, 0.01]\n",
      "[0.0889487870619946, 0.32073011734028684, 0.05428769703578862, 0.30000000000000004]\n",
      "[0.04918890633176354, 0.05833333333333329, 0.03417443358339023, 0.01]\n",
      "[0.05594405594405594, 0.10546139359698681, 0.06528434321730414, 0.4166666666666667]\n",
      "[0.26333333333333336, 0.02456140350877195, 0.06883703600204927, 0.01]\n",
      "local_EO= 0.2499508282728622\n",
      "local_EO= [0.19069069069069072, 0.32073011734028684, 0.05833333333333329, 0.4166666666666667, 0.26333333333333336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_551441/3468609132.py:7: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  tpr_sensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==1),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==1),1,0))\n",
      "/tmp/ipykernel_551441/3468609132.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  tpr_nonsensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==0),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==0),1,0))\n"
     ]
    }
   ],
   "source": [
    "vali_x_1=np.reshape(vali_x_1,(-1,28,28,3))\n",
    "vali_x_2=np.reshape(vali_x_2,(-1,28,28,3))\n",
    "vali_x_3=np.reshape(vali_x_3,(-1,28,28,3))\n",
    "vali_x_4=np.reshape(vali_x_4,(-1,28,28,3))\n",
    "vali_x_5=np.reshape(vali_x_5,(-1,28,28,3))\n",
    "\n",
    "local_EO=[]\n",
    "y_pred_1=global_model(vali_x_1)\n",
    "y_pred_1=tf.argmax(y_pred_1, axis=1)\n",
    "EO_1=EO_compuatation(y_pred_1, vali_y_1,vali_s_1)\n",
    "local_EO.append(EO_1)\n",
    "y_pred_2=global_model(vali_x_2)\n",
    "y_pred_2=tf.argmax(y_pred_2, axis=1)\n",
    "EO_2=EO_compuatation(y_pred_2, vali_y_2,vali_s_2)\n",
    "local_EO.append(EO_2)\n",
    "y_pred_3=global_model(vali_x_3)\n",
    "y_pred_3=tf.argmax(y_pred_3, axis=1)\n",
    "EO_3=EO_compuatation(y_pred_3, vali_y_3,vali_s_3)\n",
    "local_EO.append(EO_3)\n",
    "y_pred_4=global_model(vali_x_4)\n",
    "y_pred_4=tf.argmax(y_pred_4, axis=1)\n",
    "EO_4=EO_compuatation(y_pred_4, vali_y_4,vali_s_4)\n",
    "local_EO.append(EO_4)\n",
    "y_pred_5=global_model(vali_x_5)\n",
    "y_pred_5=tf.argmax(y_pred_5, axis=1)\n",
    "EO_5=EO_compuatation(y_pred_5, vali_y_5,vali_s_5)\n",
    "local_EO.append(EO_5)\n",
    "local_EO_avg=sum(local_EO)/len(local_EO)\n",
    "print('local_EO=',local_EO_avg)\n",
    "print('local_EO=',local_EO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the parameters for LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_computation(label,sensitive_attribute,y,a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))\n",
    "    N=len(label)\n",
    "    mask=(label==y)&(sensitive_attribute==a)\n",
    "    p_y_a=np.sum(mask)/N\n",
    "    return p_y_a\n",
    "S=np.zeros((5,2,4))\n",
    "p=[len(vali_x_1),len(vali_x_2),len(vali_x_3),len(vali_x_4),len(vali_x_5)]  \n",
    "for c,(feature,labels,sensitive) in enumerate(([vali_x_1,vali_y_1,vali_s_1],[vali_x_2,vali_y_2,vali_s_2],[vali_x_3,vali_y_3,vali_s_3],[vali_x_4,vali_y_4,vali_s_4],[vali_x_5,vali_y_5,vali_s_5])):\n",
    "    for a in range(2):\n",
    "        for y in range(4):\n",
    "            S[c][a][y]=p[c]*p_computation(labels,sensitive,y,a)/(len(vali_x_1)+len(vali_x_2)+len(vali_x_3)+len(vali_x_4)+len(vali_x_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_551441/1658618155.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TP_y_ac = np.sum(count_1) / np.sum(count_2)\n"
     ]
    }
   ],
   "source": [
    "TP=np.zeros((5,2,4))\n",
    "def compute_TP (y_pred,label, sensitive_attribute, y, a):\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(y_pred)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    count_1= (y_pred == y) & (label == y) & (sensitive_attribute == a)\n",
    "    count_2 = (label== y) & (sensitive_attribute == a)\n",
    "    \n",
    "    # Compute the probability\n",
    "    TP_y_ac = np.sum(count_1) / np.sum(count_2)\n",
    "    if TP_y_ac<1 or TP_y_ac==1:\n",
    "        TP_y_ac=TP_y_ac\n",
    "    else:\n",
    "        TP_y_ac=0\n",
    "    return TP_y_ac\n",
    "for c,(feature,labels,sensitive) in enumerate (([vali_x_1,vali_y_1,vali_s_1],[vali_x_2,vali_y_2,vali_s_2],[vali_x_3,vali_y_3,vali_s_3],[vali_x_4,vali_y_4,vali_s_4],[vali_x_5,vali_y_5,vali_s_5])):\n",
    "    y_pred=global_model(feature)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    for a in range(2):\n",
    "        for y in range(4):\n",
    "            TP[c][a][y]=compute_TP(y_pred,labels,sensitive,y,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(label, sensitive_attribute, y, a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(label)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    # print(mask)\n",
    "    # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "    # Compute the probability\n",
    "    alpha_y_ac = np.sum(mask) / N\n",
    "    return alpha_y_ac\n",
    "alpha_a_y=np.zeros((2,4))\n",
    "for a in range(2):\n",
    "    for y in range(4):\n",
    "        alpha_a_y[a][y]=compute_alpha(vali_y,vali_s,y,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "def LP_EO(e_0,e_c):\n",
    "    C=[]\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                C.append(-S[c][a][y])\n",
    "    ## global fairness constraints\n",
    "    A_1=[]\n",
    "    A_2=[]\n",
    "    A_3=[]\n",
    "    A_4=[]\n",
    "\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==0:\n",
    "                    A_1.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==0:\n",
    "                    A_1.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_1.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==1:\n",
    "                    A_2.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==1:\n",
    "                    A_2.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_2.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==2:\n",
    "                    A_3.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==2:\n",
    "                    A_3.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_3.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==3:\n",
    "                    A_4.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==3:\n",
    "                    A_4.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "\n",
    "                else:\n",
    "                    A_4.append(0)\n",
    "    A_1=np.array(A_1)\n",
    "    A_2=np.array(A_2)\n",
    "    A_3=np.array(A_3)\n",
    "    A_4=np.array(A_4)\n",
    "    ## define local fairness constraints\n",
    "    basis_vector=np.eye(4)\n",
    "    zero=np.zeros((4,4))\n",
    "    row_1=np.hstack((basis_vector, -basis_vector, zero, zero, zero, zero, zero, zero, zero, zero))\n",
    "    row_2=np.hstack((zero,zero, basis_vector, -basis_vector, zero, zero, zero, zero, zero, zero))\n",
    "    row_3=np.hstack((zero,zero, zero, zero, basis_vector, -basis_vector, zero, zero, zero, zero))\n",
    "    row_4=np.hstack((zero,zero, zero, zero, zero, zero, basis_vector, -basis_vector, zero, zero))\n",
    "    row_5=np.hstack((zero,zero, zero, zero, zero, zero, zero, zero, basis_vector, -basis_vector))\n",
    "    A_l=np.vstack((row_1,row_2,row_3,row_4,row_5))\n",
    "    def K_ac_compute(a,c):\n",
    "        K_ac = np.zeros((5, 4))  # 5 rows, 4 columns\n",
    "        l_ac = np.zeros((5, 1))  # 5 rows, 1 column\n",
    "\n",
    "        # Modify this line to assign only 4 elements (to match the number of columns in K_ac)\n",
    "        K_ac[0:] = [-1, -1, -1, -1]  # 4 elements, matching the number of columns\n",
    "\n",
    "        # Similarly adjust the rest of the assignments to match the column count of 4\n",
    "        K_ac[1:] = [(1 - (TP[c, a, 1] + TP[c, a, 2] + TP[c, a, 3])), TP[c, a, 0], TP[c, a, 0], TP[c, a, 0]]\n",
    "        K_ac[2:] = [TP[c, a, 1], (1 - (TP[c, a, 0] + TP[c, a, 2] + TP[c, a, 3])), TP[c, a, 1], TP[c, a, 1]]\n",
    "        K_ac[3:] = [TP[c, a, 2], TP[c, a, 2], (1 - (TP[c, a, 0] + TP[c, a, 1] + TP[c, a, 3])), TP[c, a, 2]]\n",
    "        K_ac[4:] = [TP[c, a, 3], TP[c, a, 3], TP[c, a, 3], (1 - (TP[c, a, 0] + TP[c, a, 1] + TP[c, a, 2]))]\n",
    "\n",
    "        l_ac[0:] = [-1]\n",
    "        l_ac[1:] = [TP[c, a, 0]]\n",
    "        l_ac[2:] = [TP[c, a, 1]]\n",
    "        l_ac[3:] = [TP[c, a, 2]]\n",
    "        l_ac[4:] = [TP[c, a, 3]]\n",
    "        return K_ac,l_ac\n",
    "    k_01, l_01 = K_ac_compute(0, 0)\n",
    "    k_11, l_11 = K_ac_compute(1, 0)\n",
    "    k_02, l_02 = K_ac_compute(0, 1)\n",
    "    k_12, l_12 = K_ac_compute(1, 1)\n",
    "    k_03, l_03 = K_ac_compute(0, 2)\n",
    "    k_13, l_13 = K_ac_compute(1, 2)\n",
    "    k_04, l_04 = K_ac_compute(0, 3)\n",
    "    k_14, l_14 = K_ac_compute(1, 3)\n",
    "    k_05, l_05 = K_ac_compute(0, 4)\n",
    "    k_15, l_15 = K_ac_compute(1, 4)\n",
    "\n",
    "\n",
    "    M = np.zeros((50,40))  # 10 blocks of 6x5 matrices; resulting in a 60x25 matrix\n",
    "    l = np.zeros((50, 1))   # Vector to match the 60 rows\n",
    "\n",
    "    # Place each submatrix on the diagonal\n",
    "    M[0:5, 0:4] = k_01    # Place k_01 in the top-left\n",
    "    M[5:10, 4:8] = k_11  # Place k_11 in the next diagonal block\n",
    "    M[10:15, 8:12] = k_02  # Place k_02 in the next diagonal block\n",
    "    M[15:20, 12:16] = k_12  # Place k_12 in the next diagonal block\n",
    "    M[20:25, 16:20] = k_03  # Place k_03 in the next diagonal block\n",
    "    M[25:30, 20:24] = k_13    # Next row of blocks, place k_13\n",
    "    M[30:35, 24:28] = k_04  # Place k_14 in the next block\n",
    "    M[35:40, 28:32] = k_14 # Continue placing the remaining matrices\n",
    "    M[40:45, 32:36] = k_05 # Place k_05\n",
    "    M[45:50, 36:40] = k_15 # Place k_15 in the last block\n",
    "\n",
    "    # Construct vector l\n",
    "    l[0:5] = l_01\n",
    "    l[5:10] = l_11\n",
    "    l[10:15] = l_02\n",
    "    l[15:20] = l_12\n",
    "    l[20:25] = l_03\n",
    "    l[25:30] = l_13\n",
    "    l[30:35] = l_04\n",
    "    l[35:40] = l_14\n",
    "    l[40:45] = l_05\n",
    "    l[45:50] = l_15\n",
    "    A=np.vstack((A_1,A_2,A_3,A_4,A_l,-A_1,-A_2,-A_3,-A_4,-A_l,M))\n",
    "    b_global=e_0*np.ones((4,1))\n",
    "    b_local=e_c*np.ones((20,1))\n",
    "    # print(b_local)\n",
    "    b=np.vstack((b_global,b_local,b_global,b_local,l))\n",
    "    # print(b)\n",
    "    res = linprog(C, A_ub=A, b_ub=b)\n",
    "    x=res.x\n",
    "    x=np.reshape(x,(5,2,4))\n",
    "    # print(x)\n",
    "    # print(res.fun)\n",
    "    return x\n",
    "x=LP_EO(0.01,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the LAE and generate the fair prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "vali_x_1=np.reshape(vali_x_1,(-1,28,28,3))\n",
    "vali_x_2=np.reshape(vali_x_2,(-1,28,28,3))\n",
    "vali_x_3=np.reshape(vali_x_3,(-1,28,28,3))\n",
    "vali_x_4=np.reshape(vali_x_4,(-1,28,28,3))\n",
    "vali_x_5=np.reshape(vali_x_5,(-1,28,28,3))\n",
    "beta=np.zeros((5,2,5))\n",
    "for c in range(5):\n",
    "        for a in range(2):\n",
    "            C=[0,0,0,0,0]\n",
    "            A=np.array([[1,1,1,1,1],[TP[c,a,0],1,0,0,0],[TP[c,a,1],0,1,0,0],[TP[c,a,2],0,0,1,0],[TP[c,a,3],0,0,0,1]])\n",
    "            b=np.array([1,x[c,a,0],x[c,a,1],x[c,a,2],x[c,a,3]])\n",
    "            b=np.reshape(b,(5,1))\n",
    "            res = linprog(C, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "            beta_ac=res.x\n",
    "            beta[c,a,:]=beta_ac\n",
    "            e=np.matmul(A,beta_ac)\n",
    "beta=np.reshape(beta,(5,2,5))\n",
    "\n",
    "\n",
    "\n",
    "def compute_tilde_Y(c,a, Y_hat,beta):\n",
    "    \"\"\"\n",
    "    Compute \\widetilde{Y}_{\\boldsymbol{\\beta}_{ac}}(x,a,c) based on given probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - beta_ac: Dictionary with keys 'beta_0' and 'beta_y' for probabilities.\n",
    "    - Y_hat: The predicted value \\hat{Y}(x,a,c).\n",
    "    - y_values: List or array of possible y values in \\mathcal{Y}.\n",
    "\n",
    "    Returns:\n",
    "    - tilde_Y: The computed value of \\widetilde{Y}.\n",
    "    \"\"\"\n",
    "    # Extract probabilities\n",
    "    beta_0 = beta[c, a, 0]  # Probability for Y_hat\n",
    "    beta_ac = beta[c, a, :] \n",
    "    # Probabilities for other y in \\mathcal{Y}\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    \n",
    "    # Generate a random number\n",
    "    rand_val = random.random()\n",
    "    # Determine the output based on random value\n",
    "    if rand_val < beta_0:\n",
    "        return Y_hat\n",
    "    elif rand_val < beta_0 + beta_ac[1]:\n",
    "        return 0\n",
    "    elif rand_val < beta_0 + beta_ac[1] + beta_ac[2]:\n",
    "        return 1\n",
    "    elif rand_val < beta_0 + beta_ac[1] + beta_ac[2] + beta_ac[3]:\n",
    "        return 2\n",
    "    else: # Last value\n",
    "        return 3\n",
    "\n",
    "y_tilde_1=[]\n",
    "y_tilde_2=[]\n",
    "y_tilde_3=[]\n",
    "y_tilde_4=[]\n",
    "y_tilde_5=[]\n",
    "\n",
    "# Example usage\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_x_1, vali_y_1, vali_s_1), (vali_x_2, vali_y_2, vali_s_2), (vali_x_3, vali_y_3, vali_s_3), (vali_x_4, vali_y_4, vali_s_4), (vali_x_5, vali_y_5, vali_s_5)]):\n",
    "    y_pred=global_model.predict(feature)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    for i in range (len(y_pred)):\n",
    "        a=sensitive[i]\n",
    "        y_hat=y_pred[i]\n",
    "        y_tilde=compute_tilde_Y(c,a,y_hat,beta)\n",
    "        if c==0:\n",
    "            y_tilde_1.append(y_tilde)\n",
    "        elif c==1:\n",
    "            y_tilde_2.append(y_tilde)\n",
    "        elif c==2:\n",
    "            y_tilde_3.append(y_tilde)\n",
    "        elif c==3:\n",
    "            y_tilde_4.append(y_tilde)\n",
    "        elif c==4:\n",
    "            y_tilde_5.append(y_tilde)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0010497416020671842, 0.015774397088994696, 0.0055474645836092185, 0.04]\n",
      "global_EO= 0.04\n",
      "[0.019230769230769232, 0.0, 0.04081632653061229, 0.01]\n",
      "[0.00202156334231806, 0.021512385919165582, 0.005036185188856956, 0.0]\n",
      "[0.02799581371009942, 0.020833333333333332, 0.0037735849056603765, 0.01]\n",
      "[0.015151515151515152, 0.038606403013182675, 0.0023814639736428056, 0.0]\n",
      "[0.03333333333333333, 0.04122807017543859, 0.01187648456057011, 0.01]\n",
      "local_EO= [0.04081632653061229, 0.021512385919165582, 0.02799581371009942, 0.038606403013182675, 0.04122807017543859]\n",
      "local_EO_avg_post= 0.034031799869699716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_551441/3468609132.py:7: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  tpr_sensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==1),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==1),1,0))\n",
      "/tmp/ipykernel_551441/3468609132.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  tpr_nonsensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==0),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==0),1,0))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vali_tiled_y=np.concatenate((y_tilde_1,y_tilde_2,y_tilde_3,y_tilde_4,y_tilde_5),axis=0)\n",
    "vali_tiled_y=np.reshape(vali_tiled_y,(-1,))\n",
    "global_EO_post=EO_compuatation(vali_tiled_y,vali_y,vali_s)\n",
    "print('global_EO=',global_EO_post)\n",
    "local_EO_list_post=[]\n",
    "local_EO_1=EO_compuatation(y_tilde_1,vali_y_1,vali_s_1)\n",
    "local_EO_list_post.append(local_EO_1)\n",
    "local_EO_2=EO_compuatation(y_tilde_2,vali_y_2,vali_s_2)\n",
    "local_EO_list_post.append(local_EO_2)\n",
    "local_EO_3=EO_compuatation(y_tilde_3,vali_y_3,vali_s_3)\n",
    "local_EO_list_post.append(local_EO_3)\n",
    "local_EO_4=EO_compuatation(y_tilde_4,vali_y_4,vali_s_4)\n",
    "local_EO_list_post.append(local_EO_4)\n",
    "local_EO_5=EO_compuatation(y_tilde_5,vali_y_5,vali_s_5)\n",
    "local_EO_list_post.append(local_EO_5)\n",
    "local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "print('local_EO=',local_EO_list_post)\n",
    "local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "print('local_EO_avg_post=',local_EO_avg_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_acc_post= 0.6781333333333334\n"
     ]
    }
   ],
   "source": [
    "acc_count=np.where(vali_tiled_y==vali_y,1,0)\n",
    "acc_post=sum(acc_count)/len(vali_y)\n",
    "print('global_acc_post=',acc_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_post_processing\n",
      "local_EO_avg: 0.2499508282728622\n",
      "global_EO: 0.04960709170440908\n",
      "global_acc: 0.7704\n",
      "after_post_processing\n",
      "local_EO_avg: 0.034031799869699716\n",
      "global_EO: 0.04\n",
      "global_acc: 0.6781333333333334\n"
     ]
    }
   ],
   "source": [
    "print('before_post_processing')\n",
    "print('local_EO_avg:', local_EO_avg)\n",
    "print('global_EO:', vali_EO)\n",
    "print('global_acc:', acc)\n",
    "print('after_post_processing')\n",
    "print('local_EO_avg:', local_EO_avg_post)\n",
    "print('global_EO:', global_EO_post)\n",
    "print('global_acc:', acc_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run (g,l):\n",
    "#     x=LP_EO(g,l)\n",
    "#     beta=np.zeros((5,2,5))\n",
    "#     for c in range(5):\n",
    "#             for a in range(2):\n",
    "#                 C=[0,0,0,0,0]\n",
    "#                 A=np.array([[1,1,1,1,1],[TP[c,a,0],1,0,0,0],[TP[c,a,1],0,1,0,0],[TP[c,a,2],0,0,1,0],[TP[c,a,3],0,0,0,1]])\n",
    "#                 b=np.array([1,x[c,a,0],x[c,a,1],x[c,a,2],x[c,a,3]])\n",
    "#                 b=np.reshape(b,(5,1))\n",
    "#                 res = linprog(C, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "#                 beta_ac=res.x\n",
    "#                 beta[c,a,:]=beta_ac\n",
    "#                 e=np.matmul(A,beta_ac)\n",
    "#     beta=np.reshape(beta,(5,2,5))\n",
    "#     y_tilde_1=[]\n",
    "#     y_tilde_2=[]\n",
    "#     y_tilde_3=[]\n",
    "#     y_tilde_4=[]\n",
    "#     y_tilde_5=[]\n",
    "\n",
    "#     # Example usage\n",
    "#     for c, (feature, label, sensitive) in enumerate([(vali_x_1, vali_y_1, vali_s_1), (vali_x_2, vali_y_2, vali_s_2), (vali_x_3, vali_y_3, vali_s_3), (vali_x_4, vali_y_4, vali_s_4), (vali_x_5, vali_y_5, vali_s_5)]):\n",
    "#         y_pred=global_model.predict(feature)\n",
    "#         y_pred=tf.argmax(y_pred, axis=1)\n",
    "#         for i in range (len(y_pred)):\n",
    "#             a=sensitive[i]\n",
    "#             y_hat=y_pred[i]\n",
    "#             y_tilde=compute_tilde_Y(c,a,y_hat,beta)\n",
    "#             if c==0:\n",
    "#                 y_tilde_1.append(y_tilde)\n",
    "#             elif c==1:\n",
    "#                 y_tilde_2.append(y_tilde)\n",
    "#             elif c==2:\n",
    "#                 y_tilde_3.append(y_tilde)\n",
    "#             elif c==3:\n",
    "#                 y_tilde_4.append(y_tilde)\n",
    "#             elif c==4:\n",
    "#                 y_tilde_5.append(y_tilde)\n",
    "#     vali_tiled_y=np.concatenate((y_tilde_1,y_tilde_2,y_tilde_3,y_tilde_4,y_tilde_5),axis=0)\n",
    "#     vali_tiled_y=np.reshape(vali_tiled_y,(-1,))\n",
    "#     global_EO_post=EO_compuatation(vali_tiled_y,vali_y,vali_s)\n",
    "#     local_EO_list_post=[]\n",
    "#     local_EO_1=EO_compuatation(y_tilde_1,vali_y_1,vali_s_1)\n",
    "#     local_EO_list_post.append(local_EO_1)\n",
    "#     local_EO_2=EO_compuatation(y_tilde_2,vali_y_2,vali_s_2)\n",
    "#     local_EO_list_post.append(local_EO_2)\n",
    "#     local_EO_3=EO_compuatation(y_tilde_3,vali_y_3,vali_s_3)\n",
    "#     local_EO_list_post.append(local_EO_3)\n",
    "#     local_EO_4=EO_compuatation(y_tilde_4,vali_y_4,vali_s_4)\n",
    "#     local_EO_list_post.append(local_EO_4)\n",
    "#     local_EO_5=EO_compuatation(y_tilde_5,vali_y_5,vali_s_5)\n",
    "#     local_EO_list_post.append(local_EO_5)\n",
    "#     local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "#     print('local_EO=',local_EO_list_post)\n",
    "#     local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "#     print('local_EO_avg_post=',local_EO_avg_post)\n",
    "#     acc_count=np.where(vali_tiled_y==vali_y,1,0)\n",
    "#     acc_post=sum(acc_count)/len(vali_y)\n",
    "#     print('global_EO=',global_EO_post)\n",
    "#     print('global_acc_post=',acc_post)\n",
    "#     return local_EO_avg_post,global_EO_post,acc_post\n",
    "# local_EO_avg_post,global_EO_post,acc_post=run(0.1,0.1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# def compute_confidence_interval(data, confidence=0.95):\n",
    "#     \"\"\"\n",
    "#     Compute the confidence interval for a given list of numbers.\n",
    "\n",
    "#     Parameters:\n",
    "#     data (list or array-like): List of numbers.\n",
    "#     confidence (float): Confidence level for the interval.\n",
    "\n",
    "#     Returns:\n",
    "#     tuple: Mean and the margin of error for the confidence interval.\n",
    "#     \"\"\"\n",
    "#     data = np.array(data)\n",
    "#     n = len(data)\n",
    "#     mean = np.mean(data)\n",
    "#     std_err = stats.sem(data)  # Standard error of the mean\n",
    "#     margin_of_error = std_err * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "#     return mean, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_map=[]\n",
    "# lcoal_map=[]\n",
    "# global_map=[]\n",
    "# for l in [0.05,0.1,0.15,0.2,0.25,0.3]:\n",
    "#     for g in [0.05,0.1,0.15,0.2,0.25,0.3]:\n",
    "#         acc_list=[]\n",
    "#         local_list=[]\n",
    "#         global_list=[]\n",
    "#         for _ in range (20):\n",
    "#             local_SP,post_SP,acc=run(g,l)   \n",
    "#             acc_list.append(acc)\n",
    "#             local_list.append(local_SP)\n",
    "#             global_list.append(post_SP)\n",
    "#         acc_average_5_runs=np.mean(acc_list)\n",
    "#         local_average_5_runs=np.mean(local_list)\n",
    "#         global_average_5_runs=np.mean(global_list)\n",
    "#         # mean_1, margin_1 = compute_confidence_interval(acc_list)\n",
    "#         # mean_2, margin_2 = compute_confidence_interval(local_list)\n",
    "#         # mean_3, margin_3 = compute_confidence_interval(global_list)\n",
    "\n",
    "\n",
    "#         acc_map.append(acc_average_5_runs)\n",
    "#         lcoal_map.append(local_average_5_runs)\n",
    "#         global_map.append(global_average_5_runs)\n",
    "# print(acc_map)\n",
    "# print(lcoal_map)\n",
    "# print(global_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_map=np.reshape(acc_map,(6,6))\n",
    "# local_map=np.reshape(lcoal_map,(6,6))\n",
    "# global_map=np.reshape(global_map,(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the global disparity (x-axis) and local disparity (y-axis)\n",
    "# global_disparity =[0.05,0.10,0.15,0.20,0.25,0.3]\n",
    "# local_disparity = [0.05,0.10,0.15,0.20,0.25,0.3]\n",
    "\n",
    "# # Create a 5x5 matrix representing the mean accuracy values (example data)\n",
    "# accuracy_mean_matrix = np.array(acc_map\n",
    "# )\n",
    "\n",
    "# # Create a 5x5 matrix representing the variance of accuracy values (example data)\n",
    "\n",
    "# # Create the heatmap plot\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the mean accuracy matrix using imshow\n",
    "# cax = ax.imshow(accuracy_mean_matrix, cmap='Blues', interpolation='nearest', aspect='auto', origin='lower', alpha=0.8)\n",
    "\n",
    "# # Set the labels\n",
    "# ax.set_xticks(np.arange(len(global_disparity)))\n",
    "# ax.set_yticks(np.arange(len(local_disparity)))\n",
    "# ax.set_xticklabels(global_disparity)\n",
    "# ax.set_yticklabels(local_disparity)\n",
    "\n",
    "# # Label axes and ensure they start from common 0\n",
    "# # ax.set_xlabel('Global Disparity $\\epsilon$')\n",
    "# # ax.set_ylabel('Local Disparity $\\epsilon_c$')\n",
    "\n",
    "# # Add color bar for accuracy mean\n",
    "# cbar = plt.colorbar(cax, ax=ax)\n",
    "# cbar.set_label('Avg-Acc')\n",
    "\n",
    "# # Set font size for text inside blocks\n",
    "# font_size = 8  # Adjust this value for different sizes\n",
    "\n",
    "# # Annotate each block with the mean ± variance\n",
    "# for i in range(len(local_disparity)-2):\n",
    "#     for j in range(len(global_disparity)):\n",
    "#         mean = accuracy_mean_matrix[i+2, j]\n",
    "#         ax.text(j, i+2, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='white', fontsize=14)\n",
    "# for i in range(2):\n",
    "#     for j in range(len(global_disparity)):\n",
    "#         mean = accuracy_mean_matrix[i, j]\n",
    "#         ax.text(j, i, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='black', fontsize=14)\n",
    "# for i in range(6):\n",
    "#     for j in range(1):\n",
    "#         mean = accuracy_mean_matrix[i, j]\n",
    "#         ax.text(j, i, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='black', fontsize=14)\n",
    "# for i in [2]:\n",
    "#     for j in [1]:\n",
    "#         mean = accuracy_mean_matrix[i, j]\n",
    "#         ax.text(j, i, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='black', fontsize=14)\n",
    "# # for i in range(len(local_disparity)):\n",
    "# #     for j in range(2):\n",
    "# #         mean = accuracy_mean_matrix[i, j]\n",
    "# #         ax.text(j, i, f'{mean:.3f}',\n",
    "# #                 ha='center', va='center', color='black', fontsize=12)\n",
    "\n",
    "# # Set the axes to start from a common origin\n",
    "# ax.set_xlim(-0.5, len(global_disparity) - 0.5)\n",
    "# ax.set_ylim(-0.5, len(local_disparity) - 0.5)\n",
    "\n",
    "# # Set title\n",
    "# # ax.set_title('Average Accuracy with different $(\\epsilon_0, \\epsilon_c)$')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post_FFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
